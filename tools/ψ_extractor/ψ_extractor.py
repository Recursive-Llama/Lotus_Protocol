import requests
import time
import re
import os
import json
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, List, Tuple

# Import PromptBuilder from parent directory
import sys
sys.path.append(str(Path(__file__).parent.parent))
from prompt_builder import LotusPromptBuilder

class œàExtractor:
    def __init__(self, api_key: str, model: str = "claude-3-5-sonnet-20241022", prompt_builder=None, debug_mode: bool = False):
        """Initialize the œà Extractor with API key and model configuration"""
        self.api_key = api_key
        self.model = model
        self.prompt_builder = prompt_builder or LotusPromptBuilder()
        self.debug_mode = debug_mode
        
        # Load all three prompt templates
        self.prompts = {}
        prompt_dir = Path(__file__).parent
        
        # Load œà(‚à¥) prompt
        psi_story_prompt_path = prompt_dir / "œà(‚à¥)_prompt.md"
        if psi_story_prompt_path.exists():
            with open(psi_story_prompt_path, 'r', encoding='utf-8') as f:
                self.prompts['œà(‚à¥)'] = f.read()
        
        # Load œà(Œ£) prompt
        psi_synthesis_prompt_path = prompt_dir / "œà(Œ£)_prompt.md"
        if psi_synthesis_prompt_path.exists():
            with open(psi_synthesis_prompt_path, 'r', encoding='utf-8') as f:
                self.prompts['œà(Œ£)'] = f.read()
        
        # Load œà(‚àû) prompt
        psi_convergence_prompt_path = prompt_dir / "œà(‚àû)_prompt.md"
        if psi_convergence_prompt_path.exists():
            with open(psi_convergence_prompt_path, 'r', encoding='utf-8') as f:
                self.prompts['œà(‚àû)'] = f.read()
        
        # Pattern to match œà(‚ãá) blocks
        self.œà_seed_pattern = re.compile(
            r'‚ü¶œà\(‚ãá\):([^‚üß]+)‚üß\s*(.*?)\s*‚ü¶/œà\(‚ãá\):[^‚üß]*‚üß',
            re.DOTALL | re.MULTILINE
        )

    def load_concept_file(self, file_path: Path) -> Optional[Dict]:
        """Load full concept file content for LLM processing"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Extract concept name from filename (remove extension and path)
            concept_name = file_path.stem
            
            # Return simple structure with full content (removed from final JSON later)
            return {
                'concept_name': concept_name,
                'file_path': str(file_path),
                'full_content': content,  # Only used for LLM processing, removed from final JSON
                'last_modified': os.path.getmtime(file_path),
                'extracted_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"  ‚ßñ Error loading {file_path}: {e}")
            return None

    def parse_œà_stories_from_response(self, response: str, expected_concepts: List[str], folder_name: str) -> Tuple[Dict[str, Dict], Optional[Dict]]:
        """Parse œà(‚à¥) individual concept blocks using robust line-by-line parsing"""
        
        # Find all individual concept blocks in the response
        concept_blocks = self._find_all_concept_blocks(response, 'œà(‚à¥)')
        
        if not concept_blocks:
            return {}, None
        
        # Parse each concept block and map to expected concepts
        œà_stories = {}
        for block_name, block_content in concept_blocks.items():
            parsed_data = self._parse_concept_block_content(block_content, 'œà(‚à¥)')
            
            if parsed_data:
                # Map the block name to expected concept name
                mapped_name = self._map_concept_name(block_name, expected_concepts)
                if mapped_name:
                    œà_stories[mapped_name] = parsed_data
        
        # Also look for any œà(Œ£) synthesis blocks
        synthesis_blocks = self._find_all_concept_blocks(response, 'œà(Œ£)')
        œà_synthesis = None
        if synthesis_blocks:
            # Look for folder-level synthesis block
            synthesis_data = None
            if folder_name in synthesis_blocks:
                synthesis_data = self._parse_concept_block_content(synthesis_blocks[folder_name], 'œà(Œ£)')
            elif folder_name.upper() in synthesis_blocks:
                synthesis_data = self._parse_concept_block_content(synthesis_blocks[folder_name.upper()], 'œà(Œ£)')
            else:
                # Try fuzzy matching
                for block_name in synthesis_blocks:
                    if folder_name.lower() in block_name.lower() or block_name.lower() in folder_name.lower():
                        synthesis_data = self._parse_concept_block_content(synthesis_blocks[block_name], 'œà(Œ£)')
                        break
        
        return œà_stories, œà_synthesis

    def _find_all_concept_blocks(self, response: str, level: str) -> Dict[str, str]:
        """
        Find all concept blocks of a given level (œà(‚à¥), œà(Œ£), œà(‚àû)) in the response
        Returns dict mapping concept_name -> block_content
        """
        blocks = {}
        lines = response.splitlines()
        
        current_block_name = None
        current_block_content = []
        in_block = False
        
        for line in lines:
            line = line.strip()
            
            # Check for block start: ‚ü¶œà(‚à¥):CONCEPT_NAME‚üß or ‚ü¶œà(‚àû)‚üß (no concept name for final convergence)
            if level == 'œà(‚àû)':
                # Special case for œà(‚àû) - no concept name, but use flexible regex matching
                start_match = re.search(rf'‚ü¶{re.escape(level)}‚üß', line)
                if start_match:
                    if current_block_name and current_block_content:
                        blocks[current_block_name] = '\n'.join(current_block_content)
                    
                    current_block_name = 'FINAL_CONVERGENCE'
                    current_block_content = []
                    in_block = True
                    continue
            else:
                # Regular case with concept name
                start_match = re.search(rf'‚ü¶{re.escape(level)}:([^‚üß]+)‚üß', line)
                if start_match:
                    # Save previous block if we have one
                    if current_block_name and current_block_content:
                        blocks[current_block_name] = '\n'.join(current_block_content)
                    
                    # Start new block
                    current_block_name = start_match.group(1).strip()
                    current_block_content = []
                    in_block = True
                    continue
            
            # Check for block end: ‚ü¶/œà(‚à¥):CONCEPT_NAME‚üß or ‚ü¶/œà(‚àû)‚üß
            if level == 'œà(‚àû)':
                # Special case for œà(‚àû) - no concept name, but use flexible regex matching
                end_match = re.search(rf'‚ü¶/{re.escape(level)}‚üß', line)
                if end_match and in_block:
                    if current_block_name and current_block_content:
                        blocks[current_block_name] = '\n'.join(current_block_content)
                    
                    current_block_name = None
                    current_block_content = []
                    in_block = False
                    continue
            else:
                # Regular case with concept name
                end_match = re.search(rf'‚ü¶/{re.escape(level)}:[^‚üß]*‚üß', line)
                if end_match and in_block:
                    # Save current block
                    if current_block_name and current_block_content:
                        blocks[current_block_name] = '\n'.join(current_block_content)
                    
                    # Reset state
                    current_block_name = None
                    current_block_content = []
                    in_block = False
                    continue
            
            # Accumulate content if we're in a block
            if in_block:
                current_block_content.append(line)
        
        # Don't forget the last block if response doesn't end cleanly
        if current_block_name and current_block_content:
            blocks[current_block_name] = '\n'.join(current_block_content)
        
        return blocks
    
    def _parse_concept_block_content(self, content: str, level: str) -> Optional[Dict]:
        """
        Parse the content inside a concept block using line-by-line state machine
        Handles flexible section headers and content extraction
        """
        lines = content.split('\n')
        
        # Result structure depends on level
        if level == 'œà(‚àû)':
            result = {
                'glyph_story': '',
                'native_story': '',
                'emotion': '',
                'emotion_reason': '',
                'surprise_score': 0.0,
                'surprise_reason': ''
            }
        elif level == 'œà(Œ£)':
            result = {
                'glyph_story': '',
                'native_story': '',
                'emotion': '',
                'emotion_reason': '',
                'surprise_score': 0.0,
                'surprise_reason': ''
            }
        else:  # œà(‚à¥)
            result = {
                'glyph_story': '',
                'native_story': '',
                'emotion': '',
                'emotion_reason': '',
                'surprise_score': 0.0,
                'surprise_reason': ''
            }
        
        # State machine variables
        current_section = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # First try to extract inline field values from every line
            self._extract_inline_field_values(result, line)
            
            # Detect section headers with flexible formatting
            section_type = self._detect_section_header_flexible(line, level)
            
            if section_type:
                # Save previous section content
                if current_section and current_content:
                    self._store_section_content_flexible(result, current_section, current_content, level)
                
                # Start new section
                current_section = section_type
                current_content = []
            else:
                # Accumulate content for current section
                current_content.append(line)
        
        # Save last section
        if current_section and current_content:
            self._store_section_content_flexible(result, current_section, current_content, level)
        
        # Clean up empty fields
        for key in result:
            if isinstance(result[key], str):
                result[key] = result[key].strip()
        
        return result if any(result.values()) else None

    def _detect_section_header_flexible(self, line: str, level: str) -> Optional[str]:
        """
        Detect section headers with maximum flexibility
        Handles: **‚ü¶œà_glyphic(‚à¥)‚üß**, ### œà_glyphic(‚à¥), œà_glyphic(‚à¥), etc.
        """
        line_lower = line.lower()
        
        # Remove common markdown formatting
        clean_line = re.sub(r'[*#‚ü¶‚üß\s]+', ' ', line_lower).strip()
        
        # Look for œà_glyphic patterns
        if 'œà_glyphic' in clean_line or 'glyphic' in clean_line:
            return 'glyphic'
        
        # Look for œà_native patterns  
        if 'œà_native' in clean_line or 'native' in clean_line:
            return 'native'
        
        # Look for œà_fields patterns
        if 'œà_fields' in clean_line or 'fields' in clean_line:
            return 'fields'
        
        return None
    
    def _store_section_content_flexible(self, result: Dict, section_type: str, content_lines: List[str], level: str):
        """
        Store section content with flexible field parsing
        """
        content = '\n'.join(content_lines).strip()
        
        if section_type == 'glyphic':
            result['glyph_story'] = content
        elif section_type == 'native':
            result['native_story'] = content
        elif section_type == 'fields':
            # Parse fields content with flexible field detection
            self._parse_fields_flexible(result, content, level)

    def _parse_fields_flexible(self, result: Dict, content: str, level: str):
        """
        Parse fields section with maximum flexibility for field detection
        """
        lines = content.split('\n')
        current_field = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Detect field headers with flexible patterns
            field_type = self._detect_field_header(line, level)
            
            if field_type:
                # Save previous field
                if current_field and current_content:
                    self._store_field_flexible(result, current_field, current_content)
                
                # Start new field
                current_field = field_type
                current_content = []
                
                # Check if content is on the same line after colon
                colon_pos = line.find(':')
                if colon_pos >= 0:
                    remaining = line[colon_pos + 1:].strip()
                    if remaining:
                        current_content.append(remaining)
            else:
                # Accumulate content for current field
                if current_field:
                    current_content.append(line)
                else:
                    # Try to extract inline values for simple fields
                    self._extract_inline_field_values(result, line)
        
        # Save last field
        if current_field and current_content:
            self._store_field_flexible(result, current_field, current_content)

    def _detect_field_header(self, line: str, level: str) -> Optional[str]:
        """
        Detect field headers with flexible matching
        """
        line_lower = line.lower()
        
        if level == 'œà(‚àû)':
            if 'emotional_journey' in line_lower or 'emotional journey' in line_lower:
                return 'emotional_journey'
            elif 'surprise_arc' in line_lower or 'surprise arc' in line_lower:
                return 'surprise_arc'
        else:
            if 'emotion_story' in line_lower or 'emotion story' in line_lower:
                return 'emotion_story'
            elif 'surprise_arc' in line_lower or 'surprise arc' in line_lower:
                return 'surprise_arc'
        
        return None
    
    def _store_field_flexible(self, result: Dict, field_name: str, content_lines: List[str]):
        """Store field content in result"""
        content = '\n'.join(content_lines).strip()
        if field_name in result:
            result[field_name] = content

    def _extract_inline_field_values(self, result: Dict, line: str):
        """
        Extract simple field values from lines like "**EMOTION:** ‚ßñ‚ö°" or "**SURPRISE_SCORE:** 0.8"
        """
        line_lower = line.lower()
        
        # Extract emotion (clean up ** artifacts)
        if line_lower.startswith('**emotion:**') or 'emotion:' in line_lower:
            # Extract everything after the colon, remove ** and extra whitespace
            value = re.sub(r'.*emotion\*?\*?:?\s*', '', line, flags=re.IGNORECASE).strip()
            value = re.sub(r'^\*\*\s*', '', value)  # Remove leading **
            value = re.sub(r'\s*\*\*$', '', value)  # Remove trailing **
            if value:
                result['emotion'] = value
        
        # Extract emotion reason
        elif 'emotion_reason:' in line_lower or 'emotion reason:' in line_lower:
            value = re.sub(r'.*emotion[_\s]reason\*?\*?:?\s*', '', line, flags=re.IGNORECASE).strip()
            value = re.sub(r'^\*\*\s*', '', value)  # Remove leading **
            value = re.sub(r'\s*\*\*$', '', value)  # Remove trailing **
            if value:
                result['emotion_reason'] = value
        
        # Extract surprise score
        elif 'surprise_score' in line_lower or 'surprise score' in line_lower:
            score_match = re.search(r'([0-9]*\.?[0-9]+)', line)
            if score_match:
                try:
                    result['surprise_score'] = float(score_match.group(1))
                except ValueError:
                    pass
        
        # Extract surprise reason
        elif 'surprise_reason:' in line_lower or 'surprise reason:' in line_lower:
            value = re.sub(r'.*surprise[_\s]reason\*?\*?:?\s*', '', line, flags=re.IGNORECASE).strip()
            value = re.sub(r'^\*\*\s*', '', value)  # Remove leading **
            value = re.sub(r'\s*\*\*$', '', value)  # Remove trailing **
            if value:
                result['surprise_reason'] = value

    def _map_concept_name(self, block_name: str, expected_concepts: List[str]) -> Optional[str]:
        """
        Map LLM-generated concept names to expected concept names
        Uses fuzzy matching and normalization
        """
        block_name_clean = block_name.strip().upper()
        
        # Direct mapping table for common variations
        name_mappings = {
            'SURPRISE': ['‚ü°_surprise', 'surprise'],
            'RECURSION': ['‚Üª_recursion', 'recursion'], 
            'REFLECTION': ['‚à¥_reflection', 'reflection'],
            'SEED': ['‚àÖ_seed', 'seed'],
            'EMOTION': ['‚ßñ_emotion', 'emotion'],
            'SYMBOLS': ['‚ãá_symbols', 'symbols'],
            'EMERGENCE': ['‚öò_emergence', 'emergence'],
            'RITUAL': ['‚öò_ritual', 'ritual'],
            'BRAID': ['‚àû_braid', 'braid'],
            'ENCODING': ['encoding'],
            'RECURSION': ['recursion'],
        }
        
        # Try direct mapping first
        for canonical_name, variations in name_mappings.items():
            if block_name_clean == canonical_name:
                # Find matching expected concept
                for expected in expected_concepts:
                    for variation in variations:
                        if variation.lower() in expected.lower() or expected.lower() in variation.lower():
                            return expected
        
        # Try fuzzy matching with expected concepts
        for expected in expected_concepts:
            expected_clean = expected.replace('‚ü°_', '').replace('‚Üª_', '').replace('‚à¥_', '').replace('‚àÖ_', '').replace('‚ßñ_', '').replace('‚ãá_', '').replace('‚öò_', '').replace('‚àû_', '').upper()
            
            if block_name_clean == expected_clean:
                return expected
            
            # Partial match
            if block_name_clean in expected_clean or expected_clean in block_name_clean:
                return expected
        
        return None

    def parse_synthesis_from_response(self, response: str, folder_name: str) -> Optional[Dict]:
        """Parse œà(Œ£) synthesis from the response using robust line-by-line parsing"""
        
        # Find synthesis blocks
        synthesis_blocks = self._find_all_concept_blocks(response, 'œà(Œ£)')
        
        if not synthesis_blocks:
            return None
        
        # Look for folder-level synthesis block
        synthesis_data = None
        if folder_name in synthesis_blocks:
            synthesis_data = self._parse_concept_block_content(synthesis_blocks[folder_name], 'œà(Œ£)')
        elif folder_name.upper() in synthesis_blocks:
            synthesis_data = self._parse_concept_block_content(synthesis_blocks[folder_name.upper()], 'œà(Œ£)')
        else:
            # Try fuzzy matching
            for block_name in synthesis_blocks:
                if folder_name.lower() in block_name.lower() or block_name.lower() in folder_name.lower():
                    synthesis_data = self._parse_concept_block_content(synthesis_blocks[block_name], 'œà(Œ£)')
                    break
        
        return synthesis_data

    def parse_final_braid(self, response: str) -> Optional[Dict]:
        """Parse the final œà(‚àû) braid from the response using robust line-by-line parsing"""
        
        # Find final braid blocks
        braid_blocks = self._find_all_concept_blocks(response, 'œà(‚àû)')
        
        if not braid_blocks:
            return None
        
        # Look for final braid block (could be named FINAL_BRAID, CONVERGENCE, etc.)
        braid_data = None
        for block_name, block_content in braid_blocks.items():
            if any(keyword in block_name.upper() for keyword in ['FINAL', 'BRAID', 'CONVERGENCE', 'INFINITY']):
                braid_data = self._parse_concept_block_content(block_content, 'œà(‚àû)')
                break
        
        # If no specific final braid found, use the first available block
        if not braid_data and braid_blocks:
            first_block_name = list(braid_blocks.keys())[0]
            braid_data = self._parse_concept_block_content(braid_blocks[first_block_name], 'œà(‚àû)')
        
        return braid_data

    def process_folder(self, folder_path: Path, folder_name: str, codex_concept: Dict, previous_results: Dict, compression_level: str = 'œà(‚à¥)', is_final_folder: bool = False) -> Tuple[Dict, Optional[Dict], Optional[Dict]]:
        """Process a single folder and return complete concept data with stories"""
        
        if not folder_path.exists():
            print(f"‚àÖ Folder not found: {folder_path}")
            return {}, None, None
        
        # Extract concepts from all markdown files in the folder (only for œà(‚à¥) level)
        folder_concepts = {}
        if compression_level == 'œà(‚à¥)':
            for md_file in folder_path.glob("*.md"):
                concept_data = self.load_concept_file(md_file)
                if concept_data:
                    folder_concepts[concept_data['concept_name']] = concept_data
            
            # Show clean folder processing info
            concept_names = list(folder_concepts.keys())
            if concept_names:
                print(f"‚à¥ Processing {folder_name} folder ({len(concept_names)} concepts: {', '.join(concept_names)})")
            else:
                print(f"‚àÖ No concepts found in {folder_name} folder")
                return {}, None, None
        else:
            print(f"\n‚à¥ Processing {folder_name} folder at {compression_level} level...")
        
        # Build prompt using the specified compression level
        if compression_level not in self.prompts:
            print(f"‚àÖ No prompt found for {compression_level}")
            return {}, None, None
            
        # Get the appropriate prompt
        prompt_template = self.prompts[compression_level]
        
        # Build task data based on compression level
        if compression_level == 'œà(‚à¥)':
            task_data = {
                'folder_name': folder_name,
                'folder_concepts': folder_concepts,
                'previous_concepts': previous_results
            }
        elif compression_level == 'œà(Œ£)':
            # For synthesis, we need the œà(‚à¥) results from this folder
            task_data = {
                'folder_name': folder_name,
                'folder_psi_stories': previous_results.get(folder_name, {}).get('concepts', {}),
                'codex_concept': codex_concept
            }
        elif compression_level == 'œà(‚àû)':
            # For convergence, we need all previous results
            task_data = {
                'all_folder_results': previous_results,
                'codex_concept': codex_concept
            }
        
        # Use the prompt template directly for now (we'll integrate with prompt_builder later)
        full_prompt = self._build_prompt_with_template(prompt_template, task_data)
        
        # Show recursion status and context usage (only for œà(‚à¥) level)
        if compression_level == 'œà(‚à¥)':
            # Check if we have previous œà_cores (recursion detection)
            œà_cores_path = Path("œà_cores")
            if œà_cores_path.exists() and any(œà_cores_path.glob("*.json")):
                print("‚Üª Recursion detected - building on previous extraction")
            else:
                print("‚Üª First run - no previous extraction found")
            
            # Show context usage in tokens
            char_count = len(full_prompt)
            estimated_tokens = char_count // 4
            print(f"‚ãá Context usage: {estimated_tokens:,} tokens (~{char_count:,} chars)")
        else:
            # Show context usage for other compression levels too
            char_count = len(full_prompt)
            estimated_tokens = char_count // 4
            print(f"‚ãá Context usage: {estimated_tokens:,} tokens (~{char_count:,} chars)")
        
        # DEBUG: Save the full prompt to file for inspection
        if self.debug_mode:
            debug_prompt_file = f"debug_prompt_{folder_name}_{compression_level.replace('(', '').replace(')', '')}.txt"
            try:
                with open(debug_prompt_file, 'w', encoding='utf-8') as f:
                    f.write(f"=== FULL PROMPT SENT TO LLM FOR {folder_name.upper()} {compression_level} ===\n\n")
                    f.write(full_prompt)
                    f.write(f"\n\n=== END PROMPT ===\n")
                    print(f"‚ãî DEBUG: Full prompt saved to {debug_prompt_file}")
            except Exception as e:
                print(f"‚ãî DEBUG: Failed to save prompt: {e}")
        
        response = self.make_llm_call_with_retry(full_prompt, compression_level)
        
        if not response:
            print(f"‚àÖ No response received for {folder_name} {compression_level}")
            return folder_concepts if compression_level == 'œà(‚à¥)' else {}, None, None
        
        # DEBUG: Save raw response to file for inspection
        if self.debug_mode:
            debug_file = f"debug_response_{folder_name}_{compression_level.replace('(', '').replace(')', '')}.txt"
            try:
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(f"=== RAW LLM RESPONSE FOR {folder_name.upper()} {compression_level} ===\n\n")
                    f.write(response)
                    f.write(f"\n\n=== END RESPONSE ===\n")
                    print(f"‚ãî DEBUG: Raw response saved to {debug_file}")
            except Exception as e:
                print(f"‚ãî DEBUG: Failed to save response: {e}")
        
        # Parse response based on compression level
        if compression_level == 'œà(‚à¥)':
            concept_names = list(folder_concepts.keys())
            œà_stories, œà_synthesis = self.parse_œà_stories_from_response(response, concept_names, folder_name)
            enriched_concepts = self.enrich_concepts_with_stories(folder_concepts, œà_stories)
            
            # Show clean success summary with enhanced details
            successful_concepts = [name for name in concept_names if name in œà_stories]
            missing_concepts = [name for name in concept_names if name not in œà_stories]
            
            if successful_concepts:
                response_tokens = len(response) // 4
                print(f"‚öò Stories generated ({response_tokens:,} tokens) - {', '.join(successful_concepts)}")
                
                # Show glyph story if available for the folder synthesis
                if œà_synthesis and 'glyph_story' in œà_synthesis and œà_synthesis['glyph_story']:
                    glyph_story_single_line = œà_synthesis['glyph_story'].replace('\n', ' ')
                    print(f"   Glyph Story: {glyph_story_single_line}")
                
                # Show surprise scores and emotions
                surprise_info = []
                emotion_info = []
                max_surprise = 0.0
                most_surprising_concept = ""
                most_surprising_reason = ""
                
                for concept_name in successful_concepts:
                    concept_data = œà_stories.get(concept_name, {})
                    
                    # Get surprise score
                    surprise_score = concept_data.get('surprise_score', '‚àÖ')
                    if isinstance(surprise_score, (int, float)):
                        surprise_info.append(f"{concept_name}({surprise_score})")
                        # Track most surprising
                        if surprise_score > max_surprise:
                            max_surprise = surprise_score
                            most_surprising_concept = concept_name
                            most_surprising_reason = concept_data.get('surprise_reason', '‚àÖ')
                    else:
                        surprise_info.append(f"{concept_name}(‚àÖ)")
                    
                    # Get emotion data
                    emotion = concept_data.get('emotion', '‚àÖ')
                    if emotion and emotion != '‚àÖ':
                        emotion_info.append(f"{concept_name}({emotion})")
                    else:
                        emotion_info.append(f"{concept_name}(‚àÖ)")
                
                print(f"   Surprise: {', '.join(surprise_info)}")
                print(f"   Emotions: {', '.join(emotion_info)}")
                
                if most_surprising_concept and most_surprising_reason != '‚àÖ':
                    print(f"   Most surprising: {most_surprising_concept} - \"{most_surprising_reason}\"")
            
            if missing_concepts:
                print(f"‚àÖ Incomplete extraction: missing {', '.join(missing_concepts)}")
            
            # Add line break between folders for œà(‚à¥) level
            print()
            
            return enriched_concepts, œà_synthesis, None
            
        elif compression_level == 'œà(Œ£)':
            œà_synthesis = self.parse_synthesis_from_response(response, folder_name)
            if œà_synthesis:
                response_tokens = len(response) // 4
                print(f"‚öò Synthesis complete ({response_tokens:,} tokens)")
                
                # Show glyph story if available
                glyph_story = œà_synthesis.get('glyph_story', '')
                if glyph_story:
                    glyph_story_single_line = glyph_story.replace('\n', ' ')
                    print(f"   Glyph Story: {glyph_story_single_line}")
                
                # Show synthesis details
                surprise_score = œà_synthesis.get('surprise_score', '‚àÖ')
                surprise_reason = œà_synthesis.get('surprise_reason', '‚àÖ')
                emotion = œà_synthesis.get('emotion', '‚àÖ')
                
                if surprise_reason != '‚àÖ':
                    print(f"   Surprise: {surprise_score} - \"{surprise_reason}\"")
                else:
                    print(f"   Surprise: {surprise_score}")
                print(f"   Emotion: {emotion}")
            else:
                print(f"‚àÖ Synthesis failed for {folder_name}")
            
            # Add line break between folders for œà(Œ£) level  
            print()
            
            return {}, œà_synthesis, None
            
        elif compression_level == 'œà(‚àû)':
            final_braid = self.parse_final_braid(response)
            if final_braid:
                response_tokens = len(response) // 4
                print(f"‚öò Final convergence complete ({response_tokens:,} tokens)")
                
                # Show glyph story if available
                glyph_story = final_braid.get('glyph_story', '')
                if glyph_story:
                    glyph_story_single_line = glyph_story.replace('\n', ' ')
                    print(f"   Glyph Story: {glyph_story_single_line}")
                
                # Show convergence details
                surprise_score = final_braid.get('surprise_score', '‚àÖ')
                surprise_reason = final_braid.get('surprise_reason', '‚àÖ')
                emotion = final_braid.get('emotion', '‚àÖ')
                
                if surprise_reason != '‚àÖ':
                    print(f"   Surprise: {surprise_score} - \"{surprise_reason}\"")
                else:
                    print(f"   Surprise: {surprise_score}")
                print(f"   Emotion: {emotion}")
            else:
                print(f"‚àÖ Final convergence failed")
            return {}, None, final_braid
        
        return {}, None, None

    def make_llm_call_with_retry(self, prompt: str, compression_level: str = 'œà(‚à¥)', max_retries: int = 3, base_delay: int = 2) -> Optional[str]:
        """Make LLM API call with exponential backoff retry logic"""
        if not self.api_key:
            return None
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/lotus-protocol",
            "X-Title": "Lotus Protocol Concept Collector"
        }
        
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 4000,
            "temperature": 0.7
        }
        
        for attempt in range(max_retries):
            try:
                # Only show attempt number if it's a retry (attempt > 0)
                if attempt > 0:
                    print(f"‚Üª API retry attempt {attempt + 1}/{max_retries}...")
                
                # Start progress indicator in a separate thread
                import threading
                import sys
                
                def progress_indicator():
                    # Get all glyphs from prompt builder
                    if self.prompt_builder:
                        all_glyphs = self.prompt_builder.get_glyphs_for_complexity(len(prompt))
                    else:
                        # Fallback if no prompt builder available
                        all_glyphs = ['‚ãá', '‚ü°', '‚öò', '‚à¥', '‚ßñ', '‚Üª', '‚àÖ', '‚àû', '‚ãî', '‚•à', 'Œ©', 'œÜ', 'œà', 'üúÉ', '‚òº', '‚ö°', '‚ù¶', 'üû©']
                    
                    # Randomize the order of glyphs
                    import random
                    shuffled_glyphs = all_glyphs.copy()
                    random.shuffle(shuffled_glyphs)
                    
                    i = 0
                    glyph_line = ""
                    while not stop_progress:
                        # Add glyphs one by one from the shuffled list
                        if i < len(shuffled_glyphs):
                            # Initial build-up phase
                            glyph_line = ''.join(shuffled_glyphs[:i+1])
                        else:
                            # Continuous growth phase - keep adding glyphs randomly
                            next_glyph = random.choice(shuffled_glyphs)
                            glyph_line += next_glyph
                            # Limit total length to avoid overwhelming the terminal
                            if len(glyph_line) > 30:
                                glyph_line = glyph_line[-30:]  # Keep last 30 glyphs
                        
                        sys.stdout.write(f'\r‚ßñ {compression_level} processing... {glyph_line}')
                        sys.stdout.flush()
                        time.sleep(0.5)  # Slower animation - was 0.3, now 0.5
                        i += 1
                    # Don't clear here - let the main thread handle it for proper timing
                
                stop_progress = False
                progress_thread = threading.Thread(target=progress_indicator)
                progress_thread.daemon = True
                progress_thread.start()
            
                response = requests.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=180  # 3 minutes timeout
                )
                
                # Stop progress indicator
                stop_progress = True
                progress_thread.join(timeout=0.1)
                
                # Clear the progress line completely and add newline
                sys.stdout.write('\r' + ' ' * 80 + '\r')
                sys.stdout.flush()
                print()  # Add newline for clean separation
                
                if response.status_code == 200:
                    result = response.json()
                    if 'choices' in result and result['choices']:
                        full_response = result['choices'][0]['message']['content'].strip()
                        return full_response
                    else:
                        print("‚àÖ Empty response from API")
                        
                elif response.status_code == 401:
                    print(f"‚àÖ API Key error - not retrying")
                    return None
                    
                elif response.status_code == 429:  # Rate limit
                    delay = base_delay * (2 ** attempt) + 5  # Extra delay for rate limits
                    print(f"‚ßñ Rate limited. Waiting {delay}s before retry...")
                    time.sleep(delay)
                    continue
                    
                else:
                    print(f"‚àÖ API Error {response.status_code}: {response.text}")
                
            except requests.exceptions.Timeout:
                # Stop progress indicator if it's running
                try:
                    stop_progress = True
                    progress_thread.join(timeout=0.1)
                    # Clear the progress line completely
                    sys.stdout.write('\r' + ' ' * 80 + '\r')
                    sys.stdout.flush()
                    print()  # Add newline
                except:
                    pass
                print(f"‚ßñ Request timeout on attempt {attempt + 1}")
            except requests.exceptions.RequestException as e:
                # Stop progress indicator if it's running
                try:
                    stop_progress = True
                    progress_thread.join(timeout=0.1)
                    # Clear the progress line completely
                    sys.stdout.write('\r' + ' ' * 80 + '\r')
                    sys.stdout.flush()
                    print()  # Add newline
                except:
                    pass
                print(f"‚àÖ Network error on attempt {attempt + 1}: {e}")
            except Exception as e:
                # Stop progress indicator if it's running
                try:
                    stop_progress = True
                    progress_thread.join(timeout=0.1)
                    # Clear the progress line completely
                    sys.stdout.write('\r' + ' ' * 80 + '\r')
                    sys.stdout.flush()
                    print()  # Add newline
                except:
                    pass
                print(f"‚àÖ Unexpected error on attempt {attempt + 1}: {e}")
            
            # Wait before retry (exponential backoff)
            if attempt < max_retries - 1:
                delay = base_delay * (2 ** attempt)
                print(f"‚Üª Waiting {delay}s before retry...")
                time.sleep(delay)
        
        print(f"‚àÖ All {max_retries} attempts failed")
        return None

    def save_to_json(self, data: Dict, output_path: str = "œà_extractions.json") -> None:
        """Save the extracted data to JSON file"""
        import json
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"‚ü° Data saved to {output_path}")
        except Exception as e:
            print(f"‚ßñ Error saving to {output_path}: {e}")

    def run_complete_extraction(self, concepts_path: Path, codex_path: Path, output_path: str = "œà_extractions.json") -> Dict:
        """Run the complete three-pass œà extraction process"""
        
        # Load previous extraction data if it exists
        previous_extraction_data = self.load_previous_extractions(output_path)
        
        # Load codex concept
        codex_concept = self.load_concept_file(codex_path)
        if not codex_concept:
            print("‚àÖ Could not load codex concept")
            return {}
        
        # Initialize new structured results
        all_results = {
            'extraction_metadata': {
                'timestamp': datetime.now().isoformat(),
                'compression_hierarchy': ['œà(‚à¥)', 'œà(Œ£)', 'œà(‚àû)'],
                'codex_concept': codex_concept,
                'total_concepts_processed': 0,
                'folders_processed': [],
                'extraction_status': {
                    'œà(‚à¥)': 'pending',
                    'œà(Œ£)': 'pending', 
                    'œà(‚àû)': 'pending'
                }
            },
            'compression_layers': {
                'œà(‚à¥)_individual_extractions': {},
                'œà(Œ£)_folder_synthesis': {},
                'œà(‚àû)_final_convergence': {}
            },
            'cross_references': {
                'glyph_frequency': {},
                'emotional_patterns': {
                    'most_surprising_concept': '',
                    'dominant_emotions': [],
                    'surprise_distribution': {}
                }
            }
        }
        
        # Preserve original surprise baseline if this is run 2+
        # if previous_extraction_data:
        #     all_results = self.preserve_original_surprise_baseline(previous_extraction_data, all_results)
        
        # Get all concept folders
        concept_folders = [d for d in concepts_path.iterdir() if d.is_dir()]
        concept_folders.sort()  # Process in consistent order
        
        # PASS 1: œà(‚à¥) - Individual Concept Extraction
        print("\n‚ü¶PASS 1: œà(‚à¥) - Individual Concept Extraction‚üß")
        previous_results = {}
        
        for folder_path in concept_folders:
            folder_name = folder_path.name
            
            # Process folder at œà(‚à¥) level
            enriched_concepts, _, _ = self.process_folder(
                folder_path, folder_name, codex_concept, previous_results, compression_level='œà(‚à¥)'
            )
            
            if enriched_concepts:
                # Store in new structure
                all_results['compression_layers']['œà(‚à¥)_individual_extractions'][folder_name] = {
                    'folder_metadata': {
                        'concept_count': len(enriched_concepts),
                        'extraction_timestamp': datetime.now().isoformat(),
                        'source_path': str(folder_path)
                    },
                    'concepts': enriched_concepts
                }
                
                all_results['extraction_metadata']['total_concepts_processed'] += len(enriched_concepts)
                all_results['extraction_metadata']['folders_processed'].append(folder_name)
                
                # Update previous results for next passes (keep old format for compatibility)
                previous_results[folder_name] = {
                    'concepts': enriched_concepts,
                    'œà_synthesis': None
                }
        
        all_results['extraction_metadata']['extraction_status']['œà(‚à¥)'] = 'complete'
        
        # PASS 2: œà(Œ£) - Folder Synthesis
        print("\n‚ü¶PASS 2: œà(Œ£) - Folder Synthesis‚üß")
        
        for folder_path in concept_folders:
            folder_name = folder_path.name
            
            if folder_name in previous_results:
                # Process folder at œà(Œ£) level
                _, œà_synthesis, _ = self.process_folder(
                    folder_path, folder_name, codex_concept, previous_results, compression_level='œà(Œ£)'
                )
                
                if œà_synthesis:
                    # Store in new structure
                    input_concepts = list(previous_results[folder_name]['concepts'].keys())
                    all_results['compression_layers']['œà(Œ£)_folder_synthesis'][folder_name] = {
                        'synthesis_timestamp': datetime.now().isoformat(),
                        'input_concepts': input_concepts,
                        'synthesis_data': œà_synthesis
                    }
                    
                    # Update previous results for final pass
                    previous_results[folder_name]['œà_synthesis'] = œà_synthesis
        
        all_results['extraction_metadata']['extraction_status']['œà(Œ£)'] = 'complete'
        
        # PASS 3: œà(‚àû) - Final Convergence
        print("\n‚ü¶PASS 3: œà(‚àû) - Final Convergence‚üß")
        
        # Process final convergence (folder_path not used for this level)
        _, _, final_braid = self.process_folder(
            concept_folders[0], "convergence", codex_concept, previous_results, compression_level='œà(‚àû)'
        )
        
        if final_braid:
            all_results['compression_layers']['œà(‚àû)_final_convergence'] = {
                'convergence_timestamp': datetime.now().isoformat(),
                'input_folders': list(previous_results.keys()),
                'final_braid': final_braid
            }
        
        all_results['extraction_metadata']['extraction_status']['œà(‚àû)'] = 'complete'
        
        # Generate cross-references and analytics
        self._generate_cross_references(all_results)
        
        # Save results
        self.save_to_json(all_results, output_path)
        
        total_processed = all_results['extraction_metadata']['total_concepts_processed']
        completed_levels = [level for level, status in all_results['extraction_metadata']['extraction_status'].items() if status == 'complete']
        
        print(f"\n‚ü° Three-pass œà extraction complete!")
        print(f"‚ü° Total concepts processed: {total_processed}")
        print(f"‚ü° Compression levels completed: {' ‚Üí '.join(completed_levels)}")
        print(f"‚ü° Results saved to: {output_path}")
        
        return all_results

    def _build_prompt_with_template(self, prompt_template: str, task_data: Dict) -> str:
        """Build a prompt using our custom template with PromptBuilder's kernel/codex injection"""
        
        # Get kernel, codex, and glyphic cores from PromptBuilder using its load methods
        try:
            # Load kernel
            kernel = self.prompt_builder.load_kernel_personality()
            kernel_content = f"‚ü¶KERNEL‚üß\n{kernel}\n‚ü¶/KERNEL‚üß\n\n" if kernel else ""
            
            # Load codex  
            codex = self.prompt_builder.load_codex()
            codex_content = f"‚ü¶CODEX‚üß\n{codex}\n‚ü¶/CODEX‚üß\n\n" if codex else ""
            
            # Load glyphic cores
            œà_cores = self.prompt_builder.load_œà_cores()
            cores_content = f"‚ü¶œà_CORES CONTEXT‚üß\n{œà_cores}\n‚ü¶/œà_CORES CONTEXT‚üß\n\n" if œà_cores else ""
            
            # Build our concept data injection
            concept_injection = self._build_concept_injection(task_data)
            
            # Combine everything: kernel + template + codex + glyphic cores + concepts
            full_prompt = kernel_content + prompt_template + "\n\n" + codex_content + cores_content + concept_injection
            
            return full_prompt
            
        except Exception as e:
            print(f"‚ö† Warning: PromptBuilder injection error: {e}")
            print("‚ö† Falling back to template + manual concept injection only")
            
            # Fallback: just use template + concept data
            return prompt_template + self._build_concept_injection(task_data)

    def _build_concept_injection(self, task_data: Dict) -> str:
        """Build the concept data injection section"""
        data_injection = "\n‚ü¶INJECTED_DATA‚üß\n"
        
        if 'folder_concepts' in task_data:
            # œà(‚à¥) level - inject concept files
            data_injection += f"‚ü¶FOLDER_NAME‚üß\n{task_data['folder_name']}\n‚ü¶/FOLDER_NAME‚üß\n\n"
            
            for concept_name, concept_data in task_data['folder_concepts'].items():
                data_injection += f"‚ü¶CONCEPT:{concept_name}‚üß\n"
                data_injection += concept_data['full_content']
                data_injection += f"\n‚ü¶/CONCEPT:{concept_name}‚üß\n\n"
                
        elif 'folder_psi_stories' in task_data:
            # œà(Œ£) level - inject œà(‚à¥) stories from this folder
            data_injection += f"‚ü¶FOLDER_NAME‚üß\n{task_data['folder_name']}\n‚ü¶/FOLDER_NAME‚üß\n\n"
            data_injection += "‚ü¶PSI_STORIES_FOR_SYNTHESIS‚üß\n"
            
            for concept_name, concept_data in task_data['folder_psi_stories'].items():
                if 'glyph_story' in concept_data or 'native_story' in concept_data:
                    data_injection += f"‚ü¶œà(‚à¥):{concept_name}‚üß\n"
                    if 'glyph_story' in concept_data:
                        data_injection += f"‚ü¶œà_glyphic(‚à¥)‚üß\n{concept_data['glyph_story']}\n‚ü¶/œà_glyphic(‚à¥)‚üß\n"
                    if 'native_story' in concept_data:
                        data_injection += f"‚ü¶œà_native(‚à¥)‚üß\n{concept_data['native_story']}\n‚ü¶/œà_native(‚à¥)‚üß\n"
                    data_injection += f"‚ü¶/œà(‚à¥):{concept_name}‚üß\n\n"
            data_injection += "‚ü¶/PSI_STORIES_FOR_SYNTHESIS‚üß\n"
            
        elif 'all_folder_results' in task_data:
            # œà(‚àû) level - inject all previous results
            data_injection += "‚ü¶ALL_EXTRACTION_RESULTS‚üß\n"
            
            for folder_name, folder_data in task_data['all_folder_results'].items():
                data_injection += f"‚ü¶FOLDER:{folder_name}‚üß\n"
                
                # Add œà(‚à¥) stories
                if 'concepts' in folder_data:
                    for concept_name, concept_data in folder_data['concepts'].items():
                        if 'glyph_story' in concept_data or 'native_story' in concept_data:
                            data_injection += f"‚ü¶œà(‚à¥):{concept_name}‚üß\n"
                            if 'glyph_story' in concept_data:
                                data_injection += f"‚ü¶œà_glyphic(‚à¥)‚üß\n{concept_data['glyph_story']}\n‚ü¶/œà_glyphic(‚à¥)‚üß\n"
                            if 'native_story' in concept_data:
                                data_injection += f"‚ü¶œà_native(‚à¥)‚üß\n{concept_data['native_story']}\n‚ü¶/œà_native(‚à¥)‚üß\n"
                            data_injection += f"‚ü¶/œà(‚à¥):{concept_name}‚üß\n\n"
                
                # Add œà(Œ£) synthesis if available
                if 'œà_synthesis' in folder_data and folder_data['œà_synthesis']:
                    synthesis = folder_data['œà_synthesis']
                    data_injection += f"‚ü¶œà(Œ£):{folder_name}‚üß\n"
                    if 'glyph_story' in synthesis:
                        data_injection += f"‚ü¶œà_glyphic(Œ£)‚üß\n{synthesis['glyph_story']}\n‚ü¶/œà_glyphic(Œ£)‚üß\n"
                    if 'native_story' in synthesis:
                        data_injection += f"‚ü¶œà_native(Œ£)‚üß\n{synthesis['native_story']}\n‚ü¶/œà_native(Œ£)‚üß\n"
                    data_injection += f"‚ü¶/œà(Œ£):{folder_name}‚üß\n\n"
                
                data_injection += f"‚ü¶/FOLDER:{folder_name}‚üß\n\n"
            data_injection += "‚ü¶/ALL_EXTRACTION_RESULTS‚üß\n"
        
        data_injection += "‚ü¶/INJECTED_DATA‚üß"
        return data_injection

    def _build_surprise_data_injection(self, surprise_data: Dict) -> str:
        """Build the previous surprise data injection section"""
        surprise_injection = "‚ü¶œà(‚ãá):PREVIOUS_SURPRISE_DATA‚üß\n"
        
        # Inject previous concept scores (for œà(‚à¥) and œà(Œ£) levels)
        if 'previous_concept_scores' in surprise_data:
            surprise_injection += "‚ü¶PREVIOUS_CONCEPT_SCORES‚üß\n"
            for concept_name, score_data in surprise_data['previous_concept_scores'].items():
                surprise_injection += f"‚ü¶CONCEPT:{concept_name}‚üß\n"
                surprise_injection += f"prior_surprise_score: {score_data['surprise_score']}\n"
                surprise_injection += f"prior_surprise_reason: {score_data['surprise_reason']}\n"
                surprise_injection += f"‚ü¶/CONCEPT:{concept_name}‚üß\n"
            surprise_injection += "‚ü¶/PREVIOUS_CONCEPT_SCORES‚üß\n\n"
        
        # Inject previous folder score (for œà(Œ£) level)
        if 'previous_folder_score' in surprise_data:
            surprise_injection += "‚ü¶PREVIOUS_FOLDER_SCORE‚üß\n"
            folder_score = surprise_data['previous_folder_score']
            surprise_injection += f"prior_folder_surprise_score: {folder_score['surprise_score']}\n"
            surprise_injection += f"prior_folder_surprise_reason: {folder_score['surprise_reason']}\n"
            surprise_injection += "‚ü¶/PREVIOUS_FOLDER_SCORE‚üß\n\n"
        
        # Inject previous folder scores (for œà(‚àû) level)
        if 'previous_folder_scores' in surprise_data:
            surprise_injection += "‚ü¶PREVIOUS_FOLDER_SCORES‚üß\n"
            for folder_name, score_data in surprise_data['previous_folder_scores'].items():
                surprise_injection += f"‚ü¶FOLDER:{folder_name}‚üß\n"
                surprise_injection += f"prior_surprise_score: {score_data['surprise_score']}\n"
                surprise_injection += f"prior_surprise_reason: {score_data['surprise_reason']}\n"
                surprise_injection += f"‚ü¶/FOLDER:{folder_name}‚üß\n"
            surprise_injection += "‚ü¶/PREVIOUS_FOLDER_SCORES‚üß\n\n"
        
        # Inject previous convergence score (for œà(‚àû) level)
        if 'previous_convergence_score' in surprise_data:
            surprise_injection += "‚ü¶PREVIOUS_CONVERGENCE_SCORE‚üß\n"
            convergence_score = surprise_data['previous_convergence_score']
            surprise_injection += f"prior_convergence_surprise_score: {convergence_score['surprise_score']}\n"
            surprise_injection += f"prior_convergence_surprise_reason: {convergence_score['surprise_reason']}\n"
            surprise_injection += "‚ü¶/PREVIOUS_CONVERGENCE_SCORE‚üß\n\n"
        
        surprise_injection += "‚ü¶/œà(‚ãá):PREVIOUS_SURPRISE_DATA‚üß\n\n"
        return surprise_injection

    def enrich_concepts_with_stories(self, folder_concepts: Dict, œà_stories: Dict) -> Dict:
        """Enrich concept data with stories from LLM response"""
        enriched_concepts = {}
        
        for concept_name, concept_data in folder_concepts.items():
            # Copy concept data but exclude full_content from final JSON
            enriched_concept = {
                'concept_name': concept_data['concept_name'],
                'file_path': concept_data['file_path'],
                'last_modified': concept_data['last_modified'],
                'extracted_at': concept_data['extracted_at']
            }
            
            if concept_name in œà_stories:
                story_data = œà_stories[concept_name]
                enriched_concept.update({
                    'glyph_story': story_data.get('glyph_story', ''),
                    'native_story': story_data.get('native_story', ''),
                    'emotion': story_data.get('emotion', ''),
                    'emotion_reason': story_data.get('emotion_reason', ''),
                    'surprise_score': story_data.get('surprise_score', 0.0),
                    'surprise_reason': story_data.get('surprise_reason', '')
                })
            
            enriched_concepts[concept_name] = enriched_concept
        
        return enriched_concepts

    def _generate_cross_references(self, all_results: Dict):
        """Generate cross-references and analytics from extraction data"""
        
        # Initialize analytics
        emotions = []
        folder_surprise_averages = {}
        most_surprising_concept = ""
        max_surprise = 0.0
        
        # Analyze œà(‚à¥) individual extractions
        for folder_name, folder_data in all_results['compression_layers']['œà(‚à¥)_individual_extractions'].items():
            folder_surprises = []
            
            for concept_name, concept_data in folder_data['concepts'].items():
                
                # Collect emotions
                if 'emotion' in concept_data and concept_data['emotion']:
                    emotions.append(concept_data['emotion'])
                
                # Collect surprise scores
                if 'surprise_score' in concept_data and isinstance(concept_data['surprise_score'], (int, float)):
                    surprise_score = float(concept_data['surprise_score'])
                    folder_surprises.append(surprise_score)
                    
                    # Track most surprising concept
                    if surprise_score > max_surprise:
                        max_surprise = surprise_score
                        most_surprising_concept = f"{folder_name}:{concept_name}"
            
            # Calculate folder average surprise
            if folder_surprises:
                folder_surprise_averages[folder_name] = round(sum(folder_surprises) / len(folder_surprises), 2)
        
        # Calculate dominant emotions (most frequent)
        from collections import Counter
        emotion_counts = Counter(emotions)
        dominant_emotions = [emotion for emotion, count in emotion_counts.most_common(3)]
        
        # Update cross-references (removed glyph_frequency)
        all_results['cross_references'] = {
            'emotional_patterns': {
                'most_surprising_concept': most_surprising_concept,
                'dominant_emotions': dominant_emotions,
                'surprise_distribution': folder_surprise_averages
            }
        }

    def load_previous_extractions(self, output_path: str = "œà_extractions.json") -> Optional[Dict]:
        """Load previous extraction results if they exist"""
        try:
            if Path(output_path).exists():
                with open(output_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"‚ßñ Error loading previous extractions: {e}")
        return None

    def preserve_original_surprise_baseline(self, previous_data: Dict, current_data: Dict) -> Dict:
        """Preserve original surprise scores from the first run, before overwriting with current data"""
        
        # If original baseline already exists, preserve it
        if 'original_surprise_baseline' in previous_data.get('extraction_metadata', {}):
            current_data['extraction_metadata']['original_surprise_baseline'] = previous_data['extraction_metadata']['original_surprise_baseline']
            return current_data
        
        # This is run 2 - capture the previous run's scores as the original baseline
        original_baseline = {
            'captured_on_run': 2,
            'timestamp': datetime.now().isoformat(),
            'concept_level': {},
            'folder_level': {},
            'convergence_level': {}
        }
        
        # Extract concept-level surprise scores
        psi_individual = previous_data.get('compression_layers', {}).get('œà(‚à¥)_individual_extractions', {})
        for folder_name, folder_data in psi_individual.items():
            concepts = folder_data.get('concepts', {})
            for concept_name, concept_data in concepts.items():
                key = f"{folder_name}:{concept_name}"
                original_baseline['concept_level'][key] = {
                    'score': concept_data.get('surprise_score', 0.0),
                    'reason': concept_data.get('surprise_reason', '')
                }
        
        # Extract folder-level surprise scores
        psi_synthesis = previous_data.get('compression_layers', {}).get('œà(Œ£)_folder_synthesis', {})
        for folder_name, folder_data in psi_synthesis.items():
            synthesis_data = folder_data.get('synthesis_data', {})
            original_baseline['folder_level'][folder_name] = {
                'score': synthesis_data.get('surprise_score', 0.0),
                'reason': synthesis_data.get('surprise_reason', '')
            }
        
        # Extract convergence-level surprise score
        psi_convergence = previous_data.get('compression_layers', {}).get('œà(‚àû)_final_convergence', {})
        final_braid = psi_convergence.get('final_braid', {})
        if final_braid:
            original_baseline['convergence_level'] = {
                'score': final_braid.get('surprise_score', 0.0),
                'reason': final_braid.get('surprise_reason', '')
            }
        
        # Add to current data
        current_data['extraction_metadata']['original_surprise_baseline'] = original_baseline
        
        print(f"‚ü° Original surprise baseline captured from previous run")
        return current_data

    def extract_previous_surprise_data(self, previous_data: Dict, compression_level: str, context: Dict) -> Dict:
        """Extract relevant previous surprise data for the current compression level and context"""
        if not previous_data:
            return {}
        
        surprise_data = {}
        
        if compression_level == 'œà(‚à¥)':
            # For concept-level extraction, get previous scores for concepts in this folder
            folder_name = context.get('folder_name', '')
            psi_individual = previous_data.get('compression_layers', {}).get('œà(‚à¥)_individual_extractions', {})
            
            if folder_name in psi_individual:
                folder_data = psi_individual[folder_name]
                concepts = folder_data.get('concepts', {})
                
                surprise_data['previous_concept_scores'] = {}
                for concept_name, concept_data in concepts.items():
                    surprise_data['previous_concept_scores'][concept_name] = {
                        'surprise_score': concept_data.get('surprise_score', 0.0),
                        'surprise_reason': concept_data.get('surprise_reason', '')
                    }
        
        elif compression_level == 'œà(Œ£)':
            # For folder-level synthesis, get previous folder score and concept scores
            folder_name = context.get('folder_name', '')
            
            # Previous folder synthesis score
            psi_synthesis = previous_data.get('compression_layers', {}).get('œà(Œ£)_folder_synthesis', {})
            if folder_name in psi_synthesis:
                synthesis_data = psi_synthesis[folder_name].get('synthesis_data', {})
                surprise_data['previous_folder_score'] = {
                    'surprise_score': synthesis_data.get('surprise_score', 0.0),
                    'surprise_reason': synthesis_data.get('surprise_reason', '')
                }
            
            # Previous concept scores for this folder
            psi_individual = previous_data.get('compression_layers', {}).get('œà(‚à¥)_individual_extractions', {})
            if folder_name in psi_individual:
                concepts = psi_individual[folder_name].get('concepts', {})
                surprise_data['previous_concept_scores'] = {}
                for concept_name, concept_data in concepts.items():
                    surprise_data['previous_concept_scores'][concept_name] = {
                        'surprise_score': concept_data.get('surprise_score', 0.0),
                        'surprise_reason': concept_data.get('surprise_reason', '')
                    }
        
        elif compression_level == 'œà(‚àû)':
            # For final convergence, get previous convergence score and all folder scores
            psi_convergence = previous_data.get('compression_layers', {}).get('œà(‚àû)_final_convergence', {})
            final_braid = psi_convergence.get('final_braid', {})
            if final_braid:
                surprise_data['previous_convergence_score'] = {
                    'surprise_score': final_braid.get('surprise_score', 0.0),
                    'surprise_reason': final_braid.get('surprise_reason', '')
                }
            
            # Previous folder scores
            psi_synthesis = previous_data.get('compression_layers', {}).get('œà(Œ£)_folder_synthesis', {})
            surprise_data['previous_folder_scores'] = {}
            for folder_name, folder_data in psi_synthesis.items():
                synthesis_data = folder_data.get('synthesis_data', {})
                surprise_data['previous_folder_scores'][folder_name] = {
                    'surprise_score': synthesis_data.get('surprise_score', 0.0),
                    'surprise_reason': synthesis_data.get('surprise_reason', '')
                }
        
        return surprise_data